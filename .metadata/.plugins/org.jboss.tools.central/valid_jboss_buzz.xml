<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">NEW model-specific DMN kie-server endpoints</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/U6NcGFtkqiQ/new-model-specific-dmn-kie-server-endpoints.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2021/03/new-model-specific-dmn-kie-server-endpoints.html</id><updated>2021-03-23T09:22:16Z</updated><content type="html">Modernising kie-server with new and more user-friendly DMN endpoints, better Swagger/OpenAPI documentation, easier JSON-based REST invocations; an intermediate step to help developers transitioning to service-oriented deployments such as a Kogito-based application. IN A NUTSHELL: The current DMN kie-server endpoints are fully compliant with kie-server extension design architecture, and aligned with all other kie-server services and extensions; however, some aspects of the current generic approach of kie-server sometimes are not very user-friendly for DMN evaluations, due to limitations of swagger documentation and the REST payloads requirements to follow the generic kie-server marshaller protocol. These aspects do apply to all kie-server services, including naturally DMN kie-server endpoints as well. On other hand, experience shown that building manually the REST payload on Kogito for DMN evaluation is very easy for end-users, thanks to key features pertaining to DMN core capabilities. This extends DMN on kie-server with new endpoints, leveraging those core capabilities; the new DMN endpoints provide better Swagger documentation and can be more easily consumed by end-users, therefore contributing to modernising the kie-server platform while also making easier to eventually transition to a full Kogito-based application and deployment! WHY IS THIS NEEDED? Currently on kie-server, the DMN service exposes 2 endpoints which are fully compliant with kie-server extension design architecture: * GET /server/containers/{containerId}/dmn Retrieves DMN model for given container * POST /server/containers/{containerId}/dmn Evaluates decisions for given input The current swagger documentation is agnostic to the actual model content of the knowledge asset, like for any other kie-server extension: This limited style of swagger documentation is sometimes an undesirable side-effect to the generic approach of kie-server extension design: * all kie-server extensions receive as input a generic String, which is actually converted internally to the extension using the generic kie-server marshaller. This causes the swagger documentation to not display anything meaningful for the request body besides Model==string, and the only helpful information can only be provided as a comment (“DMN context to be used while evaluation decisions as DMNContextKS type”). * all kie-server extensions return as output a ServiceResponse&lt;T&gt;, where the Java’s generic T is extension-specific. Generating swagger documentation with Java generics is already limited, in this case the difficulty compounds because the actual content of T varies, by DMN model to model ! * the DMN evaluation payload itself contains the coordinates of the model to be evaluated and the model-specific input context, per the original implementation requirements; but this interconnection between model coordinates values and input content structure, is pragmatically impossible to be defined meaningfully with a Swagger or OpenAPI descriptor. About the last point specifically, consider this example DMN payload: { "model-namespace": "https://kiegroup.org/dmn/_FA9849E2-C92E-4E27-83BF-07A7428DC9C9", "model-name": "Traffic Violation", "dmn-context": { "Driver": ..., "Violation": ... } } because the content of dmn-context depends on the values of model-namespace and model-name coordinates, there is no pragmatic way to define with Swagger/OpenAPI that dmn-context must have the properties “Driver”, “Violation” for this traffic violation model, or property “Customer” for another DMN model. Besides endpoint documentation limitations, experience proved that building manually from scratch the kie-server generic payload following the style of the kie-server generic marshaller is very difficult for most end-users (in fact we always advise to use the Kie Server Client API first, and not start from scratch, but this suggestion is often ignored anyway): * XML/JAXB format requires domain model pojo to be correctly annotated first, and building Java collection manually is quite tricky. * XML/XStream is a more natural format, still requires domain model pojo annotations, requires to respect the domain object FQN, but is yet another xml format while most end-users seem to prefer json instead. * JSON/Jackson would be the user preference nowadays, but requires to respect the domain object FQN which is very alien to json native users. Example. The correct way to marshall for Traffic Violation example, respecting the domain model defined in the kjar project, would be: { "model-namespace": "https://kiegroup.org/dmn/_FA9849E2-C92E-4E27-83BF-07A7428DC9C9", "model-name": "Traffic Violation", "dmn-context": { "Driver": { "com.acme.Driver" : { "Points": 15 }}, "Violation": { "com.acme.Violation" : { "Type": "speed", "Date": "2020-10-01", "Actual Speed": 111, "Speed Limit": 100 }} } } Everything would be much more easier, while building the JSON body payload manually for DMN evaluation, if we could drop the strict requirement to respect the generic kie-server marshalling format. NEW MODEL-SPECIFIC DMN KIE-SERVER ENDPOINTS We can now move past and beyond these limitations, thanks to the next generation of DMN endpoints on kie-server, leveraging some new DMN core capabilities: * programmatic generation of Swagger and OpenAPI (Swagger/OAS) metadata () * consistent DMNContext build from JSON, based on DMN Model metadata () to ultimately offer more user-friendly endpoints on kie-server for DMN evaluation! Following similar style to what is offered today via Kogito, summarized in this , we implemented the following new DMN endpoints on kie-server: 1. GET /server/containers/{containerId}/dmn/openapi.json (|.yaml) Retrieves Swagger/OAS for the DMN models in the kjar project 2. GET /server/containers/{containerId}/dmn/models/{modelname} Standard DMN XML but without any decision logic, so this can be used as a descriptor of the DMN model (which are the inputs, which are the decisions), while using the same format of the DMN XSD instead. 3. POST /server/containers/{containerId}/dmn/models/{modelname} JSON-only evaluation of a specific DMN model with a body payload tailored for the specific model 4. POST /server/containers/{containerId}/dmn/models/{modelname}/{decisionServiceName} JSON-only evaluation of a specific decision service of a specific DMN model with a body payload tailored for the specific model 5. POST /server/containers/{containerId}/dmn/models/{modelname}/dmnresult JSON-only evaluation of a specific DMN model with a body payload tailored for the specific model, but returning a JSON representation as a DMNResult 6. POST /server/containers/{containerId}/dmn/models/{modelname}/{decisionServiceName}/dmnresult JSON-only evaluation of a specific decision service of a specific DMN model with a body payload tailored for the specific model, but returning a JSON representation as a DMNResult For the difference between “business-domain” and “dmnresult” variants of the rest endpoints, as also linked above. Making reference to the Traffic Violation example model, this new capability can now offer on kie-server something similar to: As we can see, both the input body payload and the response body payload offer Swagger/OAS schemas which are consistent with the specific DMN model! This is possible thanks to a convergence of factors: * Because each REST POST endpoint for DMN evaluation is specific for DMN model in the REST Path, it is possible to offer Swagger/OAS definition which are DMN model-specific e.g.: because POST /server/containers/mykjar-project/dmn/traffic-violation is a REST endpoint specific to the Traffic Violation model, both its input and output payload can now be documented properly in the Swagger/OAS schema definitions. * Because each Swagger/OAS definition is offered at kjar/kie-container level, it is possible to generate programmatically the schema definitions for the DMN models contained only in the specific container. e.g.: because GET /server/containers/mykjar-project/dmn/openapi.json would offer only definitions for the DMN models inside “mykjar-project”. This is thanks to the following DMN core capability: programmatic generation of Swagger/OAS metadata () * Because these endpoints are DMN evaluation specific and focusing on a natural and idiomatic JSON usage, they do NOT require to follow the generic kie-server marshalling format. This is thanks to the following DMN core capability: consistent DMNContext build from JSON based on DMNModel metadata () ANY LIMITATIONS? Being a new set of endpoints, in addition to the currently existing ones, there is basically no impact on the already-existing DMN kie-server capabilities. As this proposed set of new endpoints are contained within a specific {containerId}, it also means that the openapi.json|.yaml swagger/OAS definition file is only kie-container specific. In turn, it means when accessing the swagger-ui client editor, user need to manually point to the container URL, for example something like: Finally, as this core capability do leverage Eclipse MicroProfile for OpenAPI Specification (OAS) and SmallRye-openapi-core, this requires making use of Swagger-UI and clients which are compatible with OpenAPI Specification version 3.0.3, onwards. CONCLUSIONS We believe this feature meaningfully extends the current set of capabilities, by providing more user-friendly DMN endpoints on kie-server! Developers can make full use of this new feature to simplify existing REST call invocations, and as a stepping stone to eventually migrate to a Kogito-based application. Have you tried it yet? Do you have any feedback? Let us know in the comments below! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/U6NcGFtkqiQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/new-model-specific-dmn-kie-server-endpoints.html</feedburner:origLink></entry><entry><title>What’s coming for Node.js developers at NearForm event</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/302eXEsoNtg/" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="Kubernetes JavaScript" /><category term="Kubernetes Node.js" /><category term="observability" /><author><name>Michael Dawson</name></author><id>https://developers.redhat.com/blog/?p=883127</id><updated>2021-03-23T07:00:34Z</updated><published>2021-03-23T07:00:34Z</published><content type="html">&lt;p&gt;Red Hat is sponsoring the very first &lt;a target="_blank" rel="nofollow" href="https://www.nearform.com/events/"&gt;NearForm Presents&lt;/a&gt; event on Mar. 31, hosted by IBM. This online event will feature four talks on interesting topics related to Node.js Core, along with exciting workshop options.&lt;/p&gt; &lt;p&gt;Our &lt;a target="_blank" rel="nofollow" href="https://github.com/nodejs"&gt;Node.js&lt;/a&gt; team is very active in the Node.js project and will present great content in these two talks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consuming New Node.js Observability Features in Kubernetes Environments (&lt;a href="https://developers.redhat.com/blog/author/aalykiot/"&gt;Alex Alykiotis&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/author/lholmqui/"&gt;Luke Holmquist&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;What’s Next, the Future of Node.js (&lt;a href="https://developers.redhat.com/blog/author/bgriggs/"&gt;Bethany Griggs&lt;/a&gt;, Joe Sepi, and &lt;a href="https://developers.redhat.com/blog/author/midawson/"&gt;Michael Dawson&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, &lt;a href="https://developers.redhat.com/blog/author/jlord/"&gt;Joel Lord&lt;/a&gt; will host a lab on Apr. 1 to help &lt;a target="_blank" rel="nofollow" href="/topics/javascript"&gt;JavaScript&lt;/a&gt; developers ramp up on &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;: Kubernetes for JS developers.&lt;/p&gt; &lt;p&gt;We hope to see you there to talk about Node.js and everything that goes with it. You can &lt;a target="_blank" rel="nofollow" href="https://ti.to/nearform/nearform-presents-nodecore-broken-promises"&gt;register for the event&lt;/a&gt; online.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#38;linkname=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F23%2Fwhats-coming-for-node-js-developers-at-nearform-event%2F&amp;#038;title=What%E2%80%99s%20coming%20for%20Node.js%20developers%20at%20NearForm%20event" data-a2a-url="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/" data-a2a-title="What’s coming for Node.js developers at NearForm event"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/"&gt;What&amp;#8217;s coming for Node.js developers at NearForm event&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/302eXEsoNtg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat is sponsoring the very first NearForm Presents event on Mar. 31, hosted by IBM. This online event will feature four talks on interesting topics related to Node.js Core, along with exciting workshop options. Our Node.js team is very active in the Node.js project and will present great content in these two talks: Consuming [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/"&gt;What&amp;#8217;s coming for Node.js developers at NearForm event&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">883127</post-id><dc:creator>Michael Dawson</dc:creator><dc:date>2021-03-23T07:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/23/whats-coming-for-node-js-developers-at-nearform-event/</feedburner:origLink></entry><entry><title>Monitor Node.js applications on Red Hat OpenShift with Prometheus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Lh6Nke4fASw/" /><category term="Containers" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="Performance" /><category term="express.js" /><category term="monitoring" /><category term="openshift" /><category term="prometheus" /><author><name>Alexandros Alykiotis</name></author><id>https://developers.redhat.com/blog/?p=876397</id><updated>2021-03-22T07:00:23Z</updated><published>2021-03-22T07:00:23Z</published><content type="html">&lt;p&gt;A great thing about &lt;a target="_blank" rel="nofollow" href="/topics/nodejs-develop-server-side-javascript-applications"&gt;Node.js&lt;/a&gt; is how well it performs inside a container. With the shift to &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containerized deployments and environments&lt;/a&gt; comes extra complexity. One such complexity is observing what’s going on within your application and its resources, and when resource use is outside of the expected norms.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; is a tool that developers can use to increase observability. It is an installable service that gathers instrumentation metrics from your applications and stores them as time-series data. Prometheus is advanced and battle-tested, and a great option for Node.js applications running inside of a container.&lt;/p&gt; &lt;h2&gt;Default and custom instrumentation&lt;/h2&gt; &lt;p&gt;For your application to feed metrics to Prometheus, it must expose a metrics endpoint. For a Node.js application, the best way to expose the metrics endpoint is to use the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/prom-client"&gt;prom-client module&lt;/a&gt; available from the Node Package Manager (NPM) registry. The &lt;code&gt;prom-client&lt;/code&gt; module exposes all of the &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/docs/instrumenting/writing_clientlibs/#standard-and-runtime-collectors"&gt;default metrics&lt;/a&gt; recommended by Prometheus.&lt;/p&gt; &lt;p&gt;The defaults include metrics such as &lt;code&gt;process_cpu_seconds_total&lt;/code&gt; and &lt;code&gt;process_heap_bytes&lt;/code&gt;. In addition to exposing default metrics, &lt;code&gt;prom-client&lt;/code&gt; allows developers to define their own metrics, as we&amp;#8217;ll do in this article.&lt;/p&gt; &lt;h2&gt;A simple Express.js app&lt;/h2&gt; &lt;p&gt;Let’s start by creating a simple &lt;a target="_blank" rel="nofollow" href="https://expressjs.com/"&gt;Express.js&lt;/a&gt; application. In this application, a service endpoint at &lt;code&gt;/api/greeting&lt;/code&gt; accepts &lt;code&gt;GET&lt;/code&gt; requests and returns a greeting as JSON. The following commands will get your project started:&lt;/p&gt; &lt;pre&gt;$ mkdir my-app &amp;#38;&amp;#38; cd my-app $ npm init -y $ npm i express body-parser prom-client &lt;/pre&gt; &lt;p&gt;This sequence of commands should create a &lt;code&gt;package.json&lt;/code&gt; file and install all of the application dependencies. Next, open the &lt;code&gt;package.json&lt;/code&gt; file in a text editor and add the following to the &lt;code&gt;scripts&lt;/code&gt; section:&lt;/p&gt; &lt;pre&gt;"start": "node app.js" &lt;/pre&gt; &lt;h3&gt;Application source code&lt;/h3&gt; &lt;p&gt;The following code is a fairly simple Express.js application. Create a new file in your text editor called &lt;code&gt;app.js&lt;/code&gt; and paste the following into it:&lt;/p&gt; &lt;pre&gt;'use strict'; const express = require('express'); const bodyParser = require('body-parser'); // Use the prom-client module to expose our metrics to Prometheus const client = require('prom-client'); // enable prom-client to expose default application metrics const collectDefaultMetrics = client.collectDefaultMetrics; // define a custom prefix string for application metrics collectDefaultMetrics({ prefix: 'my_app:' }); const histogram = new client.Histogram({ name: 'http_request_duration_seconds', help: 'Duration of HTTP requests in seconds histogram', labelNames: ['method', 'handler', 'code'], buckets: [0.1, 5, 15, 50, 100, 500], }); const app = express(); const port = process.argv[2] || 8080; let failureCounter = 0; app.use(bodyParser.json()); app.use(bodyParser.urlencoded({ extended: true })); app.get('/api/greeting', async (req, res) =&amp;#62; { const end = histogram.startTimer(); const name = req.query?.name || 'World'; try { const result = await somethingThatCouldFail(`Hello, ${name}`); res.send({ message: result }); } catch (err) { res.status(500).send({ error: err.toString() }); } res.on('finish', () =&amp;#62; { end({ method: req.method, handler: new URL(req.url, `http://${req.hostname}`).pathname, code: res.statusCode, }) ); // expose our metrics at the default URL for Prometheus app.get('/metrics', async (req, res) =&amp;#62; { res.set('Content-Type', client.register.contentType); res.send(await client.register.metrics()); }); app.listen(port, () =&amp;#62; console.log(`Express app listening on port ${port}!`)); function somethingThatCouldFail(echo) { if (Date.now() % 5 === 0) { return Promise.reject(`Random failure ${++failureCounter}`); } else { return Promise.resolve(echo); } } &lt;/pre&gt; &lt;h3&gt;Deploy the application&lt;/h3&gt; &lt;p&gt;You can use the following command to deploy the application to Red Hat OpenShift:&lt;/p&gt; &lt;pre&gt;$ npx nodeshift --expose &lt;/pre&gt; &lt;p&gt;This command creates all the OpenShift objects that your application needs in order to be deployed. After the deployment succeeds, you will be able to visit your application.&lt;/p&gt; &lt;h3&gt;Verify the application&lt;/h3&gt; &lt;p&gt;This application exposes two endpoints: &lt;code&gt;/api/greetings&lt;/code&gt; to get the greeting message and &lt;code&gt;/metrics&lt;/code&gt; to get the Prometheus metrics. First, you&amp;#8217;ll see the JSON greeting produced by visiting the &lt;code&gt;greetings&lt;/code&gt; URL:&lt;/p&gt; &lt;pre&gt;$ curl http://my-app-nodeshift.apps.ci-ln-5sqydqb-f76d1.origin-ci-int-gce.dev.openshift.com/api/greeting &lt;/pre&gt; &lt;p&gt;If everything goes well you&amp;#8217;ll get a successful response like this one:&lt;/p&gt; &lt;pre&gt;{"content":"Hello, World!"} &lt;/pre&gt; &lt;p&gt;Now, get your Prometheus application metrics using:&lt;/p&gt; &lt;pre&gt;$ curl ${your-openshift-application-url}/metrics &lt;/pre&gt; &lt;p&gt;You should be able to view output like what&amp;#8217;s shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_876577" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1.png"&gt;&lt;img aria-describedby="caption-attachment-876577" class="wp-image-876577 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-1024x611.png" alt="Prometheus metrics for a Node.js application." width="640" height="382" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-1024x611.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-300x179.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-01-1-768x458.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876577" class="wp-caption-text"&gt;Figure 1: Sample metrics from Prometheus.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Configuring Prometheus&lt;/h2&gt; &lt;p&gt;As of version 4.6, OpenShift comes with a built-in Prometheus instance. To use this instance, you will need to configure the monitoring stack and enable metrics for user-defined projects on your cluster, from an administrator account.&lt;/p&gt; &lt;h3&gt;Create a cluster monitoring config map&lt;/h3&gt; &lt;p&gt;To configure the core &lt;a target="_blank" rel="nofollow" href="/courses/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; monitoring components, you must create the &lt;code&gt;cluster-monitoring-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; object in the &lt;code&gt;openshift-monitoring&lt;/code&gt; project. Create a YAML file called &lt;code&gt;cluster-monitoring-config.yaml&lt;/code&gt; and paste in the following:&lt;/p&gt; &lt;pre&gt;apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true &lt;/pre&gt; &lt;p&gt;Then, apply the file to your OpenShift cluster:&lt;/p&gt; &lt;pre&gt;$ oc apply -f cluster-monitoring-config.yaml &lt;/pre&gt; &lt;p&gt;You also need to grant user permissions to configure monitoring for user-defined projects. Run the following command, replacing &lt;em&gt;user&lt;/em&gt; and &lt;em&gt;namespace&lt;/em&gt; with the appropriate values:&lt;/p&gt; &lt;pre&gt;$ oc policy add-role-to-user monitoring-edit &lt;em&gt;user&lt;/em&gt; -n &lt;em&gt;namespace&lt;/em&gt; &lt;/pre&gt; &lt;h3&gt;Create a service monitor&lt;/h3&gt; &lt;p&gt;The last thing to do is deploy a service monitor for your application. Deploying the service monitor allows Prometheus to scrape your application&amp;#8217;s &lt;code&gt;/metrics&lt;/code&gt; endpoint regularly to get the latest metrics. Create a file called &lt;code&gt;service-monitor.yaml&lt;/code&gt; and paste in the following:&lt;/p&gt; &lt;pre&gt;apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: nodeshift-monitor name: nodeshift-monitor namespace: nodeshift spec: endpoints: - interval: 30s port: http scheme: http selector: matchLabels: project: my-app &lt;/pre&gt; &lt;p&gt;Then, deploy this file to OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f service-monitor.yaml &lt;/pre&gt; &lt;p&gt;The whole OpenShift monitoring stack should now be configured properly.&lt;/p&gt; &lt;h2&gt;The Prometheus dashboard&lt;/h2&gt; &lt;p&gt;With OpenShift 4.6, the Prometheus dashboard is integrated with OpenShift. To access the dashboard, go to your project and choose the &lt;b&gt;Monitoring&lt;/b&gt; item on the left, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_876537" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02.png"&gt;&lt;img aria-describedby="caption-attachment-876537" class="wp-image-876537 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-1024x515.png" alt="CPU usage in the Prometheus dashboard." width="640" height="322" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-1024x515.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-02-768x386.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876537" class="wp-caption-text"&gt;Figure 2: Prometheus monitoring in the OpenShift dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To view the Prometheus metrics (using &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/docs/prometheus/latest/querying/basics/"&gt;PromQL&lt;/a&gt;), go to the second tab called &lt;b&gt;Metrics&lt;/b&gt;. You can query and graph any of the metrics your application provides. For example, Figure 3 graphs the size of the heap.&lt;/p&gt; &lt;div id="attachment_876547" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03.png"&gt;&lt;img aria-describedby="caption-attachment-876547" class="wp-image-876547 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-1024x515.png" alt="A heap graph in the Prometheus dashboard." width="640" height="322" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-1024x515.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-03-768x386.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876547" class="wp-caption-text"&gt;Figure 3: A heap graph in Prometheus.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Testing the application&lt;/h2&gt; &lt;p&gt;Next, let&amp;#8217;s use the &lt;a target="_blank" rel="nofollow" href="https://httpd.apache.org/docs/2.4/programs/ab.html"&gt;Apache Bench&lt;/a&gt; tool to add to the load on our application. We&amp;#8217;ll hit our API endpoint 10,000 times with 100 concurrent requests at a time:&lt;/p&gt; &lt;pre&gt;$ ab -n 10000 -c 100 http://my-app-nodeshift.apps.ci-ln-5sqydqb-f76d1.origin-ci-int-gce.dev.openshift.com/api/greeting &lt;/pre&gt; &lt;p&gt;After generating this load, we can go back to the main Prometheus dashboard screen and construct a simple query to see how the service performed. We&amp;#8217;ll use our custom &lt;code&gt;http_request_duration_seconds&lt;/code&gt; metric to measure the average request duration during the last five minutes. Type this query into the textbox:&lt;/p&gt; &lt;pre&gt;rate(http_request_duration_seconds_sum[5m])/rate(http_request_duration_seconds_count[5m])&lt;/pre&gt; &lt;p&gt;Then, go to the Prometheus dashboard to see the nicely drawn graph shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_876567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04.png"&gt;&lt;img aria-describedby="caption-attachment-876567" class="wp-image-876567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-1024x645.png" alt="Performance monitoring with Prometheus." width="640" height="403" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-1024x645.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-300x189.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-04-768x484.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-876567" class="wp-caption-text"&gt;Figure 4: Results from a custom query.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We get two lines of output because we have two types of responses: The successful one (200) and the server error (500). We can also see that as the load increases, so does the time required to complete HTTP requests.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has been a quick introduction to monitoring Node.js applications with Prometheus.  You’ll want to do much more for a production application, including setting up alerts and adding custom metrics to support &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture/blob/master/docs/operations/metrics.md#guidance"&gt;RED metrics&lt;/a&gt;. But I’ll leave those options for another article. Hopefully, this was enough to get you started and ready to learn more.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our new &lt;a target="_blank" rel="nofollow" href="/topics/nodejs-develop-server-side-javascript-applications"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#38;linkname=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F22%2Fmonitor-node-js-applications-on-red-hat-openshift-with-prometheus%2F&amp;#038;title=Monitor%20Node.js%20applications%20on%20Red%20Hat%20OpenShift%20with%20Prometheus" data-a2a-url="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/" data-a2a-title="Monitor Node.js applications on Red Hat OpenShift with Prometheus"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/"&gt;Monitor Node.js applications on Red Hat OpenShift with Prometheus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Lh6Nke4fASw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;A great thing about Node.js is how well it performs inside a container. With the shift to containerized deployments and environments comes extra complexity. One such complexity is observing what’s going on within your application and its resources, and when resource use is outside of the expected norms. Prometheus is a tool that developers can [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/"&gt;Monitor Node.js applications on Red Hat OpenShift with Prometheus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">876397</post-id><dc:creator>Alexandros Alykiotis</dc:creator><dc:date>2021-03-22T07:00:23Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/22/monitor-node-js-applications-on-red-hat-openshift-with-prometheus/</feedburner:origLink></entry><entry><title type="html">Point of sale - An architectural introduction</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Ssw22L1RVU8/point-of-sale-an-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/E9rgT9xWToA/point-of-sale-an-architectural-introduction.html</id><updated>2021-03-22T06:00:00Z</updated><content type="html">This article kicks off the first in the series sharing another new architecture blueprint. It's focusing on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge creating architectural content based on common customer adoption patterns. That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Architectural Blueprint.  Let's look at these blueprints, how their created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division in to smaller parts. In this case we've aligned with the higher level theme being 'Retail' use cases, a vertical focus. This breaks down into the following use cases and in no particular order: * * * * Headless eCommerce * Store health and safety * Real-time stock control * Retail data framework The case I'm tackling here is focused on point of sale images. This use case we've defined as the following: Simplifying and modernizing central management of distributed point-of-sale devices with built in support for container based applications. The approach taken is to research our existing customers that have implemented solutions in this space, collect their public facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  To get an idea of what these blueprints look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architectural blueprint for use telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on business optimisation portfolio architecture blueprint: 1. 2. Common architectural elements 3. Example image distribution architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at the generic common architectural elements for the supply chain integration architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Ssw22L1RVU8" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/E9rgT9xWToA/point-of-sale-an-architectural-introduction.html</feedburner:origLink></entry><entry><title>Managing Python dependencies with the Thoth JupyterLab extension</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lFgYvP4zKGE/" /><category term="Machine Learning" /><category term="Microservices" /><category term="Open source" /><category term="Python" /><category term="Jupyter notebook" /><category term="JupyterLab" /><category term="Project Thoth" /><category term="Python dependencies" /><author><name>Francesco Murdaca</name></author><id>https://developers.redhat.com/blog/?p=855857</id><updated>2021-03-19T07:00:12Z</updated><published>2021-03-19T07:00:12Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/jupyterlab/jupyterlab"&gt;JupyterLab&lt;/a&gt; is a flexible and powerful tool for working with Jupyter notebooks. Its interactive user interface (UI) lets you use terminals, text editors, file browsers, and other components alongside your Jupyter notebook. JupyterLab 3.0 was released in January 2021.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; develops &lt;a target="_blank" rel="nofollow" href="/topics/open-source"&gt;open source&lt;/a&gt; tools that enhance the day-to-day lives of developers and data scientists. Thoth uses machine-generated knowledge to boost your applications&amp;#8217; performance, security, and quality through reinforcement learning with &lt;a target="_blank" rel="nofollow" href="/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt;. (&lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=WEJ65Rvj3lc&amp;#38;t=1s"&gt;Watch this video&lt;/a&gt; to learn more about resolving dependencies with reinforcement learning.)&lt;/p&gt; &lt;p&gt;This machine learning approach is implemented in &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth adviser&lt;/a&gt;, a recommendation engine for Python applications. &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/integration.rst"&gt;Thoth integrations&lt;/a&gt; use this knowledge to provide software stack recommendations based on &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos#using-custom-configuration-file-template"&gt;user inputs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article introduces you to &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt;, a JupyterLab extension for managing and optimizing Python dependencies in your Jupyter notebooks. As you will learn, using the &lt;code&gt;jupyterlab-requirements&lt;/code&gt; extension is a smart and easy way to ensure that your code and experiments are always reproducible.&lt;/p&gt; &lt;h2&gt;Making application dependencies reproducible&lt;/h2&gt; &lt;p&gt;When creating code or conducting experiments, reproducibility is an important requirement. Ensuring that others can rerun experiments in the same environment the creator used is critical, especially when developing machine learning applications.&lt;/p&gt; &lt;p&gt;Let’s consider one of the first steps for developing an application: specifying dependencies. For example, your project might depend on &lt;a target="_blank" rel="nofollow" href="https://pandas.pydata.org/docs/getting_started/index.html"&gt;pandas&lt;/a&gt; for data exploration and manipulation or &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/tensorflow/"&gt;TensorFlow&lt;/a&gt; for training a model.&lt;/p&gt; &lt;p&gt;One approach to this task is to run a command in the notebook cell to install the dependencies directly on the host, as shown in Figure 1. This way, the next user can run the same cell and install similar packages.&lt;/p&gt; &lt;div id="attachment_855977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png"&gt;&lt;img aria-describedby="caption-attachment-855977" class="wp-image-855977" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png" alt="A user installing dependencies in the notebook cell using the pip install command." width="640" height="102" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png 848w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16-300x48.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16-768x122.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-855977" class="wp-caption-text"&gt;Figure 1: Installing dependencies directly on the host using the &lt;code&gt;pip install&lt;/code&gt; command.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Another potential strategy is to provide a &lt;code&gt;requirements.txt&lt;/code&gt; file that lists all of the dependencies so that someone else can install them before starting the notebook. Figure 2 shows an example.&lt;/p&gt; &lt;div id="attachment_855987" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png"&gt;&lt;img aria-describedby="caption-attachment-855987" class="wp-image-855987" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png" alt="A sample list of software dependencies in a text file." width="640" height="102" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png 1131w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-300x48.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-768x123.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-1024x164.png 1024w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-855987" class="wp-caption-text"&gt;Figure 2: A sample list of dependencies.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Do you see any issues with these two approaches to specifying dependencies?&lt;/p&gt; &lt;p&gt;Neither one supports reproducibility!&lt;/p&gt; &lt;p&gt;In the first scenario, let&amp;#8217;s say another user tried to rerun the same cell sometime after a new version of the library was released. They might experience different behavior from the initial notebook output.&lt;/p&gt; &lt;p&gt;The same issue can arise with the &lt;code&gt;requirements.txt&lt;/code&gt; file, only with the package names. Even if you stated the direct dependencies with the exact version number, each of those dependencies might depend on other so-called &lt;em&gt;transitive dependencies&lt;/em&gt; that are also installed.&lt;/p&gt; &lt;p&gt;To guarantee reproducibility, you must account for all dependencies with specific version numbers for direct and transitive dependencies, including all hashes used to verify the provenance of the packages for security reasons (check these &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/docs/developers/adviser/provenance_checks.html"&gt;docs&lt;/a&gt; to learn more about security in software stacks). To be even more precise, the Python version, operating system, and hardware all influence the code’s behavior. You should share all of this information so other users can experience the same behavior and obtain similar results.&lt;/p&gt; &lt;p&gt;Project Thoth aims to help you specify direct and transitive dependencies so that your applications are always reproducible and you can focus on more pressing challenges.&lt;/p&gt; &lt;h2&gt;Dependency management with jupyterlab-requirements&lt;/h2&gt; &lt;p&gt;The Thoth team has introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt;, a JupyterLab extension for dependency management that is currently focused on the &lt;a target="_blank" rel="nofollow" href="/blog/category/python/"&gt;Python&lt;/a&gt; ecosystem. This extension lets you manage your project&amp;#8217;s dependencies directly from a Jupyter notebook, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_856057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg"&gt;&lt;img aria-describedby="caption-attachment-856057" class="wp-image-856057" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg" alt="Screenshot of the jupyterlab-requirements extension with the Managed Dependencies menu item highlighted." width="640" height="202" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg 1950w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-300x95.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-768x243.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-1024x323.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856057" class="wp-caption-text"&gt;Figure 3: Managing dependencies in JupyterLab.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;When you click &lt;strong&gt;Manage Dependencies&lt;/strong&gt;, you will see the dialog box shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_856077" style="width: 523px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-856077" class="wp-image-856077 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00.png" alt="A dialog box stating ‘No dependencies found! Click button above to add new packages.’" width="513" height="243" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00.png 513w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00-300x142.png 300w" sizes="(max-width: 513px) 100vw, 513px" /&gt;&lt;p id="caption-attachment-856077" class="wp-caption-text"&gt;Figure 4: A new notebook with no dependencies identified.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Initially, the extension will not identify any dependencies when you start a new notebook; it checks the notebook metadata to detect them. You can add your packages by clicking the button with the plus-sign (+) icon, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_856087" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png"&gt;&lt;img aria-describedby="caption-attachment-856087" class="wp-image-856087" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png" alt="Screenshot of the Manage Dependencies screen with the option to add new package dependencies." width="640" height="291" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png 794w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48-300x136.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48-768x349.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856087" class="wp-caption-text"&gt;Figure 5: Adding new packages with the jupyterlab-requirements extension.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After saving, an &lt;strong&gt;Install&lt;/strong&gt; button will appear. You can check the package names and versions before installing the dependencies, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_856097" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png"&gt;&lt;img aria-describedby="caption-attachment-856097" class="wp-image-856097" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png" alt="Screenshot of the Manage Dependencies screen with the Install button and sample packages added." width="640" height="427" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png 841w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42-300x200.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42-768x512.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856097" class="wp-caption-text"&gt;Figure 6: The Manage Dependencies screen with sample packages added and ready to install.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After clicking Install, you will see the screen shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_856107" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png"&gt;&lt;img aria-describedby="caption-attachment-856107" class="wp-image-856107" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png" alt="Screenshot of the Manage Dependencies screen with requirements locked, saved, and installed." width="640" height="361" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png 788w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12-300x169.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12-768x433.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856107" class="wp-caption-text"&gt;Figure 7: The packages are locked, saved, and installed in the notebook metadata.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;All dependencies—both direct and transitive—will be locked, saved in the notebook metadata, and installed. What’s more, the extension automatically creates and sets the kernel for your notebook. No human intervention is necessary, and you are ready to work on your project.&lt;/p&gt; &lt;h2&gt;Managing dependencies in an existing notebook&lt;/h2&gt; &lt;p&gt;If you have existing notebooks with code, you can still use the &lt;code&gt;jupyterlab-requirements&lt;/code&gt; extension to share them. The &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/invectio/"&gt;invectio&lt;/a&gt; library analyzes code in the notebook and suggests libraries that must be installed to run the notebook. Figure 8 shows an example.&lt;/p&gt; &lt;div id="attachment_882847" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05.png"&gt;&lt;img aria-describedby="caption-attachment-882847" class="wp-image-882847 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-1024x501.png" alt="There are no dependencies in the notebook metadata, but the extension identifies three packages to be installed." width="640" height="313" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-1024x501.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-300x147.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-768x376.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-882847" class="wp-caption-text"&gt;Figure 8: The &lt;code&gt;invectio&lt;/code&gt; utility identifies three packages required to run the notebook.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once again, you can just install the dependencies and start working on your project.&lt;/p&gt; &lt;h2&gt;Locking dependencies with Thoth or Pipenv&lt;/h2&gt; &lt;p&gt;The resolution engine you use to lock dependencies provides two files: a &lt;code&gt;Pipfile&lt;/code&gt; and a &lt;code&gt;Pipfile.lock&lt;/code&gt;. The &lt;code&gt;Pipfile.lock&lt;/code&gt; file states all direct and transitive project dependencies with specific versions and hashes. The notebook metadata stores these files and information about the Python version, operating system, and hardware detected. This way, anyone using the same notebook can re-create the environment that the original developer used.&lt;/p&gt; &lt;p&gt;Two resolution engines are available at the moment: &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Thoth&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/pypa/pipenv"&gt;Pipenv&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Currently, Thoth is used by default, with Pipenv as a backup. This setup guarantees that the user will receive the software stack to work on their projects. In the future, users will be able to select a specific resolution engine.&lt;/p&gt; &lt;p&gt;Using the Thoth resolution engine, you can request an optimized software stack that satisfies your requirements from the Thoth recommendation system. You can choose from the following recommendation types according to your particular needs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Latest&lt;/li&gt; &lt;li&gt;Performance&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Stable&lt;/li&gt; &lt;li&gt;Testing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information about the various &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/recommendation-types/"&gt;recommendation types&lt;/a&gt;, visit the Project Thoth website.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: The notebook metadata stores which resolution engine was used so that anyone can immediately see which one was used to resolve dependencies.&lt;/p&gt; &lt;h3&gt;Configuring the runtime environment&lt;/h3&gt; &lt;p&gt;You don’t need to worry about the runtime environment when using the Thoth resolution engine. Thoth automatically identifies the information needed to generate a recommendation and creates a &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos"&gt;Thoth configuration file&lt;/a&gt; containing the following parameters:&lt;/p&gt; &lt;pre&gt;host: {THOTH_SERVICE_HOST} tls_verify: true requirements_format: {requirements_format} runtime_environments: - name: '{os_name}:{os_version}' operating_system: name: {os_name} version: '{os_version}' hardware: cpu_family: {cpu_family} cpu_model: {cpu_model} gpu_model: {gpu_model} python_version: '{python_version}' cuda_version: {cuda_version} recommendation_type: stable platform: '{platform}'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: If you use the Thoth resolution engine, the notebook metadata will also contain information about the runtime environment used for the notebook. In this way, other data scientists using the notebook will be warned about using a different one.&lt;/p&gt; &lt;h3&gt;Installing dependencies and creating the kernel&lt;/h3&gt; &lt;p&gt;Once a lock file is created using either Thoth or Pipenv, the &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/micropipenv/"&gt;micropipenv&lt;/a&gt; tool installs the dependencies in the virtual environment. The &lt;code&gt;micropipenv&lt;/code&gt; tool supports dependency management in Python and beyond (&amp;#8220;one library to rule them all&amp;#8221;).&lt;/p&gt; &lt;p&gt;Once all of the dependencies are installed in your kernel, you are ready to work on your notebook.&lt;/p&gt; &lt;p&gt;You can choose the name of the new kernel and select the requirements from the drop-down menu. Once everything is installed, the kernel is assigned to the current notebook automatically.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt; extension is an open source project maintained by the Thoth team. We are currently exploring new features for the UI, and we welcome anyone who would like to contribute or give us feedback about the extension.&lt;/p&gt; &lt;p&gt;Have a look at the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements/issues"&gt;open issues&lt;/a&gt; and get in touch with the team if you like the project or if you find any issues with the extension. The Thoth team also has a &lt;a target="_blank" rel="nofollow" href="https://chat.google.com/room/AAAAVjnVXFk"&gt;public channel&lt;/a&gt; where you can ask questions about the project. We are always happy to collaborate with the community on any of &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station"&gt;our repositories&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#038;title=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" data-a2a-url="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/" data-a2a-title="Managing Python dependencies with the Thoth JupyterLab extension"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;Managing Python dependencies with the Thoth JupyterLab extension&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lFgYvP4zKGE" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;JupyterLab is a flexible and powerful tool for working with Jupyter notebooks. Its interactive user interface (UI) lets you use terminals, text editors, file browsers, and other components alongside your Jupyter notebook. JupyterLab 3.0 was released in January 2021. Project Thoth develops open source tools that enhance the day-to-day lives of developers and data scientists. [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;Managing Python dependencies with the Thoth JupyterLab extension&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">855857</post-id><dc:creator>Francesco Murdaca</dc:creator><dc:date>2021-03-19T07:00:12Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/</feedburner:origLink></entry><entry><title type="html">Kogito Tooling 0.8.5 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/azFdIzrpPvI/kogito-tooling-0-8-5-released.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/03/kogito-tooling-0-8-5-released.html</id><updated>2021-03-19T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! On the 0.8.5 , we made many improvements and bug fixes. We are also happy to announce a new PMML Scorecard Editor and, also, that our editors are now available on Eclipse Theia Upstream (built from ). This post will give a quick overview of what is included on this release. PMML SCORECARD EDITOR (ALPHA) HITS VSCODE MARKET PLACE We are happy to announce that we have a new VS Code extension: . It allows you to create and edit PMML 4.4 (.pmml) Scorecard files. This new editor is in the alpha stage, and we are looking for feedback from the community. We hope you enjoy it! ECLIPSE THEIA AND OPEN VSIX STORE Eclipse Theia is an extensible framework based on VS Code to develop full-fledged multi-language Cloud &amp;amp; Desktop IDE-like products with state-of-the-art web technologies. Recently, Theia’s team merged a , allowing support for CustomEditor API. In practice, this means that from now on, our BPMN, DMN and editors can run on Eclipse Theia upstream (you can build it from and run), take a look on this demo: Eclipse Theia uses , and from now on, all our releases will also be available on Open VSX store. NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS We also made some new features, a lot of refactorings and improvements, with highlights to: NEW FEATURES: INFRASTRUCTURE * – Implement a integration tests using Cypress for online channel * – Migrate VS Code Extension release job to new Jenkins instance * – Converge the CSS to avoid conflicts between PF3 and PF4 EDITORS * – Score Cards: Integrate with VS Code channel * – Enable Bpmn and Dmn PR tests * – Run standalone tests with Chrome instead of Electron FIXED ISSUES IN KOGITO: INFRASTRUCTURE * – Fix running online editor integration tests in CI EDITORS * – Importing and modeling decision models is too slow for productive modeling * – [DMN Designer] Decision Services – The parameters order in the properties panel is not correct * – DMN Editor wrong edge arrow tip connection on reopen * – [DMN Designer] DMN schema/model validation errors when model has AUTO-SOURCE or AUTO-TARGET connections * – Scesim assets are broken in VS Code extension * – [DMN Designer] DMN takes too long to open models with too many nodes FURTHER READING/WATCHING We had some excellent talks recently at the KIE youtube channel: * Building successful business Java apps: How to deliver more, code less, and communicate better, by Alex; * How to embed DMN and BPMN editors in your own application, by Paulo; * Using VSCode to build and deploy services in a real-world decision scenario: COVID-19, by Adriel/ I would also like to recommend some recent articles: * , by Guilherme; * , by Matteo; * , by Eder. THANK YOU TO EVERYONE INVOLVED! I want to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look fabulous! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/azFdIzrpPvI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/kogito-tooling-0-8-5-released.html</feedburner:origLink></entry><entry><title type="html">WildFly Bootable JAR 4.0 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VEQ0cjVV9Yo/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/03/19/bootable-jar-4.0.Final-Released/</id><updated>2021-03-19T00:00:00Z</updated><content type="html">The 4.0.0.Final version of the has been released. For people who are not familiar with the WildFly Bootable JAR, I strongly recommend that you read this that covers it in detail. WILDFLY CLI SCRIPT EXECUTED AT STARTUP TIME Starting with the recently WildFly 23, the bootable JAR runtime allows you to execute a WildFly CLI script when launching the bootable JAR. Although applying changes to the server configuration at build time is the preferred way (no impact on startup time), runtime execution gives you the flexibility to adjust the server configuration to the execution context. The example has been evolved with a new 'runtime-config' Maven profile to disable CLI script execution at build time in favor of runtime execution. To execute a CLI script during boot: java -jar app-bootable.jar --cli-script=&lt;path to CLI script&gt; Note This support is Tech Preview as the mechanism may change in later releases. JBOSS MODULES MODULE ARTIFACT UPGRADES Starting with WildFly 23, you can upgrade part of the server when building a bootable JAR. This offers you the ability to use a different version of an identified component of the server (eg: an Undertow artifact, a JDBC driver provided by a third party Galleon feature-pack, … ). Obviously the updated component must be compatible with the server in which it is provisioned… This is done at your own risk ;-). WILDFLY GALLEON FEATURE-PACK SERVER ARTIFACT PACKAGING The way the JBoss Modules module artifacts that compose a WildFly server are packaged inside a WildFly Galleon feature-pack allows you to override their version when building a Bootable JAR. Instead of packaging the artifact binaries inside Galleon feature-packs, the artifacts' Maven coordinates are packaged. The actual artifact files are resolved when a WildFly server is built using Galleon (or when building a bootable JAR). This way of packaging artifacts is not new; WildFly follows this design pattern since the first Galleon releases. (This is what allows you to provision a slim WildFly server or a ). If you design custom Galleon feature-packs for WildFly, we encourage you, when applicable (the artifacts must be released in an accessible Maven repository), to design your feature-packs by following this pattern. As an example, the (that provides drivers and datasources for some major databases) pom file contains in its the driver artifacts that it can bring to the server. The JBoss Modules modules that contain the driver artifacts (e.g. ) only reference the GroupId and ArtifactId of the artifact. The artifact versions are stored inside the feature-pack but outside of the JBoss Modules module. This separation between the GroupId, ArtifactId (optionally Classifier) and the Version is what makes it possible to upgrade when building a bootable JAR. TO LEARN MORE ABOUT ARTIFACT UPGRADES The contains more information about this capability. MYFACES GALLEON FEATURE-PACK I’m happy to take the opportunity of this blog post to mention a new community that defines Galleon feature-packs that you can use with the WildFly 23 Galleon feature-pack (and "WildFly Preview" Galleon feature-pack) to build a Bootable JAR (or to provision a server using Galleon) containing a JSF implementation based on . KNOWN ISSUES INCOMPATIBILITY WITH KEYCLOAK CLIENT ADAPTER GALLEON FEATURE-PACK Due to some incompatible changes in the WildFly Galleon feature-pack (i.e. the removal of core and servlet Galleon feature-packs in the dependency chain) the Keycloak 12.0.x OIDC client adapter Galleon feature-pack can’t be used with the WildFly 23 Galleon feature-pack. As a workaround, we have setup a that highlights the workaround you need to follow to include the Keycloak 12.0.x OIDC client adapter inside a WildFly 23 bootable JAR. In summary, we are including in the bootable JAR the content of the Keycloak zipped client adapter (that you can download from ). In addition the WildFly server security is configured by the adapter-elytron-install.cli WildFly CLI script that is packaged in the zipped adapter. TO CONCLUDE Finally we would really appreciate if if you would keep us posted with your feedback and new requirements. (You can log these as new .) This will help us evolve the WildFly Bootable JAR experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VEQ0cjVV9Yo" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/19/bootable-jar-4.0.Final-Released/</feedburner:origLink></entry><entry><title type="html">RESTful Services Orchestration with Kogito and OpenAPI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aEiq4QHW-eU/restful-services-orchestration-with-kogito-and-openapi.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2021/03/restful-services-orchestration-with-kogito-and-openapi.html</id><updated>2021-03-18T19:07:06Z</updated><content type="html">The invocation of remote services plays a big role in workflow orchestration. In this blog post, we will take a look at RESTful service orchestration using and the OpenAPI specification. CNCF SERVERLESS WORKFLOW IMPLEMENTATION Kogito is a modern business automation runtime. In addition to flowchart and form-based workflow DSLs, it also supports , a declarative workflow DSL that targets the serverless technology domain. At the time of this writing, Kogito supports a subset of features of the . Since version 1.3.0, Kogito has the ability to define workflows that can via OpenAPI. This fits well with the Serverless Workflow specification, where OpenAPI is the default standard for describing RESTful services. In other words, you don’t need to worry about writing boilerplate client code to orchestrate RESTful services. All you need to do is to declare the service calls! UNDERSTANDING FUNCTION DECLARATIONS WITH OPENAPI Our business requirement is to write a simple serverless temperature converter. To do this, we want to write a workflow that can orchestrate two existing RESTful services, namely Multiplication and Subtraction services. These two services are described via OpenAPI, meaning they are described in a programming language-agnostic way. This means that you do not need to know how to write the code that invokes these services. Kogito reads this function definition during build time. It contains the needed information to generate REST client code based on these OpenAPI specification files during build time. The code generated is based on the , now embedded in our platform. In our workflow definition, we have to know the location of the services’ OpenAPI definition and the specific operation we want to invoke on the defined service. Serverless Workflow allows us to define reusable function definitions. These definitions represent an invocation of an operation on a remote service. Function definitions have a domain-specific name and can be referenced by that name throughout workflow control-flow logic when they need to actually be invoked. Below is our workflow function definition we will use throughout the blog post: "functions": [ { "name": "multiplication", "operation": "specs/multiplication.yaml#doOperation" }, { "name": "subtraction", "operation": "specs/subtraction.yaml#doOperation" } ] CALLING YOUR RESTFUL SERVICES With this in place, invoking these services in the workflow becomes trivial. All we have to do is define when in the workflow control-flow logic they need to be invoked. Workflow control-flow logic in Serverless Workflow is defined within the "states" block. This is where you define all your workflow states (steps) and the transitions between them: "states": [ { "name": "Computation", "actionMode": "sequential", "type": "operation", "actions": [ { "name": "subtract", "functionRef": { "refName": "subtraction", "parameters": { "subtractionOperation": { "leftElement": "$.fahrenheit", "rightElement": "$.subtractValue" } } } }, { "name": "multiply", "functionRef": { "refName": "multiplication", "parameters": { "multiplicationOperation": { "leftElement": "$.subtraction.difference", "rightElement": "$.multiplyValue" } } } } ] } ] Going back to our business requirements, for temperature conversion, our workflow needs to call the two services in a certain order. First, we want to execute the Multiplication service and then the Subtraction service. The is perfect for what we need. The parameters are taken from the workflow data input and processed with JSONPath expressions. And how do we know how to define these parameters? It’s just a matter of extracting from the OpenAPI Specification file: # ... operationId: doOperation requestBody: content: application/json: schema: $ref: '#/components/schemas/MultiplicationOperation' # ... components: schemas: MultiplicationOperation: type: object properties: leftElement: format: float type: number product: format: float type: number rightElement: format: float type: number The workflow declares two functions that represent the service operations that should be invoked during workflow execution. The first one, multiplication, will execute the operation doOperation from the OpenAPI specification file in our project’s classpath (Kogito also supports file and http schemas). Same thing for the subtraction function. Since this operation only needs one parameter, we can name it as we like (in this case, multiplicationOperation). For operations that require multiple parameters (like query strings), you should use the same names as defined by the OpenAPI specification. CONFIGURING THE ENDPOINTS The last piece of this puzzle is to define the URL for each of the services we want to invoke. To do so, declare the URLs in your application properties file. You should set a configuration like: org.kogito.openapi.client.&lt;spec file name&gt;.base_path=. This is a runtime property that can be defined using any method the target runtime (Quarkus or SpringBoot) supports. But if the OpenAPI Specification file declares the endpoint URL, you don’t even need to bother. Take a look at the , for example: host: petstore.swagger.io basePath: v2 schemas: - http - https Now you’re ready to call your newly generated Kogito Workflow and start orchestrating services! You can find the full Temperature Conversion workflow example . TIP: If you’re curious about the CNCF Serverless Workflow Kogito implementation, please take a look at these references: * Blog Post: ;  * Video: . If you have any questions about this new feature, please drop a question on . We would love to hear from you! * Featured photo by on The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aEiq4QHW-eU" height="1" width="1" alt=""/&gt;</content><dc:creator>Ricardo Zanini</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/restful-services-orchestration-with-kogito-and-openapi.html</feedburner:origLink></entry><entry><title type="html">How to setup the OpenShift Container Platform 4.7 on your local machine</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rdkXVj8xLsw/codeready-containers-howto-setup-openshift-47-on-local-machine.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/oyjB7WzMTt0/codeready-containers-howto-setup-openshift-47-on-local-machine.html</id><updated>2021-03-18T06:00:00Z</updated><content type="html"> Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform? Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.7? Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud native development and automated rolling deployments.  The idea was to make this as streamlined of an experience as possible by using the same  project. Let's take a look at what this looks like. Below is a walk through step by step, putting the latest OpenShift Container Platform on your local developer machine. LINUX OR MAC INSTALLATION This installation requires the following (all freely available): &gt; 1. HyperKit for OSX, Hyper-V for Windows, or Libvirt for Linux &gt; 2. Code Ready Containers (OCP 4.7) &gt; 3. OpenShift Client (oc) v4.7 First you need to ensure your virtualization tooling is installed for your platform, just search online for how to do that or your specific platform. Second you need to download the CodeReady Containers. Finally, you need the OpenShift client. Normally you'd expect to have to track these last two down but we've made this all easy by just including checks during the installation. If you have something installed, it checks the version, if good then it moves on with next steps. If anything is missing or the wrong version, the installation stops and notifies you where to find that component for your platform (including URL). Let's get started by downloading the  project and unzipping in some directory. This gives you a file called ocp-install-demo-main.zip,just unzip and run the init.sh as follows:      $ ./init.sh  Follow the instructions as each of the dependencies is checked and you're provided with pointers to getting the versions you need for your platform. Note: Each CodeReady Container download is tied to an embedded secret. This secret you need to download (link will be provided) as a file and you'll be asked to point to that secret to start your container platform. Once you've gotten all the dependencies sorted out, the install runs like this: A little ASCII art and then it's checking for my platform's virtualization (Hyperkit), then looking for the OpenShift client version (oc client), then running a setup (crc setup). The next steps are providing the pull-secret-file, you can set this in the variables at the top of the installation script. Now the moment of truth, the CodeReady Containers cluster starts, which takes some time depending on your network (crc start). With a good network it's about a five minute wait. This is the logging you'll see as the OpenShift cluster starts on your local machine. The warning is normal, just some of the features have been trimmed to speed up deployment. At the end we'll retrieve the admin password for logging in to the cluster's console, pick up the host URL, test the deployment by logging in with our client (oc login), and finally you're given all the details in a nice box. You have the option to stop, start it again, or delete the OpenShift Container Platform cluster as shown in the dialog. Next open the web console using URL and login 'kubeadmin' with the corresponding password. In our case it's the URL: https://console-openshift-console.apps-crc.testing Login with user: kubeadmin Password in our case: duduw-yPT9Z-hsUpq-f3pre That opens the main dashboard: Verify the version you are running by clicking on the top right question mark and then About option: Close the version window by clicking on the X. As we are interested in developing using the tooling and container images provided by CodeReady Containers, let's change the view from Administrator to Developer in the left top menu selecting Topology and then via Project drop down menu at the top choose Default: You can browse the offerings in the provided container catalog by selecting From Catalog and then for example, Middleware to view the offerings available: Looking to get started with an example usage, try the  or examples that leverage the provided developer catalog container images. You can also explore how an existing project is set up using one of the developer catalog container images with a . This concludes the installation and tour of an OpenShift Container Platform on our local machine using CodeReady Containers. WHAT ABOUT WINDOWS? If you are a sharp observer, you'll notice there is a file called init.bat for windows platforms to install with. The problem is I've not been able to test this yet on a windows machine, so I'd love to call out to the readers out there that might have some time to contribute to test this script and help us complete the installation. You'll notice a few TODO's marked in the scripts code, as they are untested areas in the installation. You can  and help us complete the windows based installation and get your name added to the contributors list. We'd be really thankful! If you are interested in the , see this post for details on how to install. Stay tuned for more on cloud-native development using other Red Hat technologies on your new OpenShift Container Platform installed locally on your own machine!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rdkXVj8xLsw" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/oyjB7WzMTt0/codeready-containers-howto-setup-openshift-47-on-local-machine.html</feedburner:origLink></entry><entry><title type="html">WildFly 23 S2I images have been released on quay.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Q3FBK4FtNY8/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/03/18/WildFly-s2i-23-Released/</id><updated>2021-03-18T00:00:00Z</updated><content type="html">WILDFLY 23 S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 23 have been released on . For a complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . ADDING THE IMAGESTREAMS AND TEMPLATE TO OPENSHIFT At some point the new images will be made available from the OpenShift catalog and image repository. But you can already use these images by adding them yourselves to your OpenShift cluster. * WildFly S2I builder image stream: oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-23.0/imagestreams/wildfly-centos7.json * WildFly runtime image stream: oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-23.0/imagestreams/wildfly-runtime-centos7.json * Chained build template: oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-23.0/templates/wildfly-s2i-chained-build-template.yml NB: If you import the image streams into your project, be sure to set the ImageStreams Namespace (IMAGE_STREAM_NAMESPACE argument) to your project namespace in the template. openshift being the default namespace. Enjoy!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Q3FBK4FtNY8" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/18/WildFly-s2i-23-Released/</feedburner:origLink></entry></feed>
